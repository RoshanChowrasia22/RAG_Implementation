{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c879503",
   "metadata": {},
   "source": [
    "### RAG Pipelines- Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d6377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import GPT2TokenizerFast\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d0a41",
   "metadata": {},
   "source": [
    "### Read all the pdf's inside the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "88a312dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files to process\n",
      "\n",
      "Processing: paint  (1).pdf\n",
      "  ✓ Loaded 2 pages\n",
      "\n",
      "Processing: paint  (2).pdf\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "                doc.metadata['time_loading']=datetime.now().strftime('%Y-%d-%m')#strftime('%Y-%d-%m')\n",
    "                doc.metadata['iteration_number']=2\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96748c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(766,\n",
       " Document(metadata={'producer': 'Acrobat Distiller 6.0 (Windows)', 'creator': 'Adobe Acrobat 6.0', 'creationdate': '2006-10-18T12:52:36+08:00', 'author': 'Christopher M. Bishop', 'moddate': '2008-02-08T16:41:33+01:00', 'title': 'Pattern Recognition and Machine Learning', 'source': '..\\\\data\\\\pdf\\\\paint  (2).pdf', 'total_pages': 758, 'page': 0, 'page_label': 'i', 'source_file': 'paint  (2).pdf', 'file_type': 'pdf', 'time_loading': '2025-15-11', 'iteration_number': 2}, page_content=''))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pdf_documents),all_pdf_documents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c405f",
   "metadata": {},
   "source": [
    "### Text splitting get into chunks using characters and token as wwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fab6b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,# Characters\n",
    "        chunk_overlap=chunk_overlap,#Characters\n",
    "        # length_function=len,#charcater based\n",
    "        length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=False)),# LLM tokenizer\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],# First split by paragraph then new lines then spaces and then character wise\n",
    "        strip_whitespace=True\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7b75a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 766 documents into 772 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: ROSHAN CHOWRASIA ♂phone+91-8334913863\n",
      "M.Tech. /envel⌢peroshanchowrasia12@gmail.com\n",
      "Computational and Data Sciences (CDS) Indian Institute Of Science, Bangalore\n",
      "/linkedinLinkedIn Profile\n",
      "Summary\n",
      "Skille...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-28T17:00:39+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-28T17:00:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\paint  (1).pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'paint  (1).pdf', 'file_type': 'pdf', 'time_loading': '2025-15-11', 'iteration_number': 2}\n"
     ]
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d838bf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational and Data Sciences (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020\\nB.Sc. in Computer Science Percentage: 86.17\\nExperience\\n•Cloudcraftz 15th April 2025 - Present\\nData Scientist Kolkata\\nOptimize and Reconcile Live strategies:\\n–Worked on optimization of a multi-signal trading strategy incorporating EMA-based signals, RSI\\nsignals, and stop-loss/take-profit mechanisms.\\n–Implemented Genetic Algorithm for parameter optimization, where the fitness function was defined as\\nthe ratio of mean returns to standard deviation (µ/σ), effectively balancing profitability and risk.\\n–Reconcile strategy performance between backtested results and live execution, identifying and\\nexplaining sources of deviation\\n–Apply quantitative and statistical methods to evaluate strategy robustness under varying market\\nconditions\\n•Bajaj Auto 22nd July 2024 - 28th March 2025\\nData Scientist Pune\\nWarranty Data Model and Visualization:\\n–Mapped the entire warranty process and contributed to creating a comprehensive warranty data model,\\nensuring alignment with the warranty team.\\n–Developed a warranty data model, visualizations, and KPIs to streamline warranty processes and\\nsupport data-driven decision-making.\\n–Designed and executed SQL workflows to truncate and load warranty data, enabling dashboard\\nvisualizations for dealer and state-wise acceptance and rejection trends.\\n–KPIs derived to reflect warranty acceptance and rejection percentages, providing actionable insights to\\nsupport informed decision-making via dashboard.\\n–The reports generated via dashboard resulted in 2 hours per day savings.\\nData Governance Clustering for SPs:\\n–Designed a clustering approach using CBOW embeddings and K-Means to group SPs by business unit\\nfor improved data governance and security.\\n–Developed a clustering strategy to group SPs (Stored Procedures) such that each cluster aligned with a\\nspecific business unit, ensuring relevant table access and preventing data misuse.',\n",
       " '–Implemented data transformations and CBOW (Continuous Bag of Words) embeddings to capture\\nsemantic relationships, followed by K-Means clustering to form 10 balanced clusters.\\n–Achieved uniform SP distribution across clusters, ensuring secure and controlled data access for\\npartners.\\n•Bajaj Auto 22nd May 2023 - 22nd July 2023\\nOctane Intern Pune\\nRetail Simulation & Power BI Integration\\n–Performed a time series analysis for retail using SARIMA and integrated the results into Power BI\\ndashboards to support business decisions.\\n–Developed an end-to-end pipeline using Python and Power BI, extracting data from Azure Synapse,\\npreprocessing it, and delivering insights through dynamic Power BI dashboards.\\n–Simulated retail activities based on booking, inquiries, and test rides data, leveraging Python and\\nPower BI to create \"what-if\" analysis dashboards for enhanced decision-making.\\nPersonal Projects\\n•Anomalous chemical shift yielding amino acid classification using Protein Sequence (MTech Project):2023-2024\\nMachine learning-based approach to predict anomalous chemical shifts in NMR spectroscopy, achieving 98% accuracy.\\n– Designed a Random Forest-based method to predict anomalous chemical shifts using amino acid sequences and\\nHomology Derived Structure of Proteins (HSSP) data.\\n– Deployed an odd-sized sliding window approach to model local interactions between neighboring amino acids and\\ntheir chemical environments.\\n– Achieved an accuracy of 98%, F1-score of 0.98, and ROC AUC of 0.99, enabling efficient pre-processing in the\\nNMR pipeline.\\n•Image Segmentation Using K-Means Clustering: 2022\\nImage segmentation from scratch using K-Means clustering, processing raw image data without built-in libraries.\\n– Processed raw image data, applied clustering to segment regions based on color intensity, and optimized the\\nalgorithm for performance.\\n– Demonstrated strong programming and mathematical skills by independently building and debugging the algo-\\nrithm.\\n•Image Compression Using Singular Value Decomposition (SVD): 2022\\nImage compression using SVD, reducing image size by retaining significant singular values while preserving visual quality.\\n– Applied Singular Value Decomposition (SVD) independently to the red, green, and blue channels of images for\\ncompression.\\n– Retained the top k singular values in each channel, optimizing for minimal perceptual difference from the original\\nimage.\\n•Animal sound Recognition 2020\\nBuild a model to generate spectrograms from .wav files and use CNN for feature extraction and classification using DNN\\n– Developed a deep learning pipeline to classify animal sounds from 8 species by converting .wav files into spectro-\\ngrams for visual representation.\\n– Leveraged Convolutional Neural Networks (CNN) to extract meaningful features from spectrogram images and\\ntrained a Deep Neural Network (DNN) for classification.\\n– Achieved a classification accuracy of 78.3%, ’demonstrating the model’s effectiveness in recognizing distinct animal\\nsounds.\\nTechnical Skills and Interests\\nLanguages:Python, C++, Structured Query Language (SQL).\\nData Analysis: Statistical Analysis, Predictive Modelling, Time Series Analysis, Business Analytics.\\nData Handling: Pandas, NumPy, SQL.\\nVisualization Tools: Power BI, Matplotlib, Seaborn.\\nAnalytical Tools: Excel,Power BI.\\nAchievements\\n• Selected in Bajaj A cricket team as a medium fast bowler. 2024\\n• Secured an All India Rank of 185 in the GATE CS paper. 2022\\n• First in Digi-bit hosted at St Xavier’s College Kolkata. 2018\\n• Second in Enigma (coding event) hosted by SIGMA at St Xavier’s College Kolkata. 2018\\n• Recipient of School Topper Award and Principals’ Choice Award in the year 2017. 2017',\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-28T17:00:39+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-28T17:00:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\paint  (1).pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'paint  (1).pdf', 'file_type': 'pdf', 'time_loading': '2025-15-11', 'iteration_number': 2}, page_content='ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational and Data Sciences (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020\\nB.Sc. in Computer Science Percentage: 86.17\\nExperience\\n•Cloudcraftz 15th April 2025 - Present\\nData Scientist Kolkata\\nOptimize and Reconcile Live strategies:\\n–Worked on optimization of a multi-signal trading strategy incorporating EMA-based signals, RSI\\nsignals, and stop-loss/take-profit mechanisms.\\n–Implemented Genetic Algorithm for parameter optimization, where the fitness function was defined as\\nthe ratio of mean returns to standard deviation (µ/σ), effectively balancing profitability and risk.\\n–Reconcile strategy performance between backtested results and live execution, identifying and\\nexplaining sources of deviation\\n–Apply quantitative and statistical methods to evaluate strategy robustness under varying market\\nconditions\\n•Bajaj Auto 22nd July 2024 - 28th March 2025\\nData Scientist Pune\\nWarranty Data Model and Visualization:\\n–Mapped the entire warranty process and contributed to creating a comprehensive warranty data model,\\nensuring alignment with the warranty team.\\n–Developed a warranty data model, visualizations, and KPIs to streamline warranty processes and\\nsupport data-driven decision-making.\\n–Designed and executed SQL workflows to truncate and load warranty data, enabling dashboard\\nvisualizations for dealer and state-wise acceptance and rejection trends.\\n–KPIs derived to reflect warranty acceptance and rejection percentages, providing actionable insights to\\nsupport informed decision-making via dashboard.\\n–The reports generated via dashboard resulted in 2 hours per day savings.\\nData Governance Clustering for SPs:\\n–Designed a clustering approach using CBOW embeddings and K-Means to group SPs by business unit\\nfor improved data governance and security.\\n–Developed a clustering strategy to group SPs (Stored Procedures) such that each cluster aligned with a\\nspecific business unit, ensuring relevant table access and preventing data misuse.'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content,chunks[1].page_content,chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2672125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2338, 980)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks),len(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41f8b5",
   "metadata": {},
   "source": [
    "### Sentence level chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb938b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b493305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 766 documents into 2530 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: roshan chowrasia [UNK] + 91 - 8334913863 m. tech. / [UNK] @ gmail. com computational and data sciences ( cds ) indian institute of science, bangalore / linkedinlinkedin profile summary skilled data sc...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-28T17:00:39+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-28T17:00:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\paint  (1).pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'paint  (1).pdf', 'file_type': 'pdf', 'time_loading': '2025-15-11', 'iteration_number': 2}\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size=512,        # Max tokens per chunk\n",
    "    chunk_overlap=64,      # Token overlap\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "chunks_sentence_level=split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b01377fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2530"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_sentence_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd95a49",
   "metadata": {},
   "source": [
    "### Paragraph level chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaaa164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def token_len(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0896cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 766 documents into 776 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: ROSHAN CHOWRASIA ♂phone+91-8334913863\n",
      "M.Tech. /envel⌢peroshanchowrasia12@gmail.com\n",
      "Computational and Data Sciences (CDS) Indian Institute Of Science, Bangalore\n",
      "/linkedinLinkedIn Profile\n",
      "Summary\n",
      "Skille...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-08-28T17:00:39+00:00', 'author': '', 'keywords': '', 'moddate': '2025-08-28T17:00:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\paint  (1).pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'paint  (1).pdf', 'file_type': 'pdf', 'time_loading': '2025-15-11', 'iteration_number': 2}\n"
     ]
    }
   ],
   "source": [
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],   # Paragraph first\n",
    "    chunk_size=1024,                     # Max characters (or use token fn)\n",
    "    chunk_overlap=200,\n",
    "    length_function=token_len,#len                 # Change to token fn for precision\n",
    "    keep_separator=True                  # Keeps \\n\\n in chunk\n",
    "    )\n",
    "    split_docs = paragraph_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "chunks_paragraph_level=split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b7ec4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_paragraph_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04690e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1 [51 tokens]:\n",
      "Machine learning is a field of AI. It enables systems to learn from data. [¶] Models are trained on large datasets. They improve over time through optimization. [¶] This process is known as training. It involves backpropagation and gradient descent.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Machine learning is a field of AI. It enables systems to learn from data.\n",
    "\n",
    "Models are trained on large datasets. They improve over time through optimization.\n",
    "\n",
    "This process is known as training. It involves backpropagation and gradient descent.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64,\n",
    "    length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=False))\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1} [{len(tokenizer.encode(c))} tokens]:\")\n",
    "    print(c.replace(\"\\n\\n\", \" [¶] \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2555ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "['ro', '##shan', 'is', 'trying', 'to', 'find', 'out', 'what', 'is', 'token', '##izer', '.', 'en', '##code', 'doing', '.']\n",
      "['R', 'osh', 'an', ' is', ' trying', ' to', ' find', ' out', ' what', ' is', ' token', 'izer', '.', 'en', 'code', ' doing', '.']\n"
     ]
    }
   ],
   "source": [
    "w=\"Roshan is trying to find out what is tokenizer.encode doing.\"\n",
    "D=[wrd for wrd in w if wrd!=' ']\n",
    "print(len(D))\n",
    "F=tokenizer.encode(w, add_special_tokens=False)\n",
    "tokens=[tokenizer.decode(id) for id in F]\n",
    "print(tokens)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "F2=tokenizer.encode(w, add_special_tokens=False)\n",
    "tokens=[tokenizer.decode(id) for id in F2]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1f717",
   "metadata": {},
   "source": [
    "### Semnatic level encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6a493e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1:\n",
      "Machine learning is a subset of AI. It enables systems to learn from data. \n",
      "Deep learning uses neural networks. These networks have many layers.\n",
      "\n",
      "Training requires large datasets. GPUs speed up computation. \n",
      "Cloud platforms like AWS are popular.\n",
      "\n",
      "Evaluation uses metrics like accuracy. Precision and recall are important.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        similarity_threshold=0.75,\n",
    "        max_chunk_tokens=512\n",
    "    ):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = similarity_threshold\n",
    "        self.max_tokens = max_chunk_tokens\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=0,\n",
    "            separators=[\". \", \"! \", \"? \", \"\\n\", \" \"]\n",
    "        )\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def _token_count(self, text: str) -> int:\n",
    "        return len(self.tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    def chunk(self, text: str) -> List[str]:\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        if len(sentences) <= 1:\n",
    "            return sentences\n",
    "\n",
    "        # Encode all sentences\n",
    "        embeddings = self.model.encode(sentences, normalize_embeddings=True)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        current_emb = embeddings[0]\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity([current_emb], [embeddings[i]])[0][0]\n",
    "            \n",
    "            # Check token limit\n",
    "            temp_chunk = \" \".join(current_chunk + [sentences[i]])\n",
    "            if self._token_count(temp_chunk) > self.max_tokens:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentences[i]]\n",
    "                current_emb = embeddings[i]\n",
    "                continue\n",
    "\n",
    "            # Add if similar enough\n",
    "            if sim >= self.threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "                # Update centroid\n",
    "                chunk_embs = self.model.encode(current_chunk, normalize_embeddings=True)\n",
    "                current_emb = np.mean(chunk_embs, axis=0)\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentences[i]]\n",
    "                current_emb = embeddings[i]\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Machine learning is a subset of AI. It enables systems to learn from data. \n",
    "Deep learning uses neural networks. These networks have many layers.\n",
    "\n",
    "Training requires large datasets. GPUs speed up computation. \n",
    "Cloud platforms like AWS are popular.\n",
    "\n",
    "Evaluation uses metrics like accuracy. Precision and recall are important.\n",
    "\"\"\"\n",
    "\n",
    "chunker = SemanticChunker(similarity_threshold=0.7)\n",
    "chunks_ex = chunker.chunk(text)\n",
    "\n",
    "for i, c in enumerate(chunks_ex):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "caea89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# splitter = SemanticSplitterNodeParser(\n",
    "#     buffer_size=1,\n",
    "#     breakpoint_percentile_threshold=0.95,   # merge if similarity > 95th percentile\n",
    "#     embed_model=embed_model\n",
    "# )\n",
    "# nodes = splitter.get_nodes_from_documents(text)\n",
    "# chunks = [n.text for n in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe92ea",
   "metadata": {},
   "source": [
    "### embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3ae3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "543614c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x15e2e167e90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c9e3b",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c276d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 11690\n",
      "Collection(name=pdf_documents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x273c56ef3d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            print(self.collection)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d5d2c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning is a subset of AI. It enables systems to learn from data. \\nDeep learning uses neural networks. These networks have many layers.\\n\\nTraining requires large datasets. GPUs speed up computation. \\nCloud platforms like AWS are popular.\\n\\nEvaluation uses metrics like accuracy. Precision and recall are important.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029647b",
   "metadata": {},
   "source": [
    "### Convert the text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bde24ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2338\n",
      "Generating embeddings for 2338 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 74/74 [04:43<00:00,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (2338, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "print(len(texts))\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc511d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2338"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c1e14",
   "metadata": {},
   "source": [
    "### store int he vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12864c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 2338 documents to vector store...\n",
      "Successfully added 2338 documents to vector store\n",
      "Total documents in collection: 11690\n"
     ]
    }
   ],
   "source": [
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "841e1bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "686917b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_len=[len(s) for s in texts]\n",
    "max(texts_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acd10",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore --> Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f7b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager : EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if distance >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "351730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x273c7096e50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7e78529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Tell me about Roshan Chowrasia IISc Bangalore Computational Data Science'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_2c900366_2317',\n",
       "  'content': 'ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational Data Science (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020',\n",
       "  'metadata': {'subject': '',\n",
       "   'title': '',\n",
       "   'time_loading': '2025-28-10',\n",
       "   'source': '..\\\\data\\\\pdf\\\\paint  (3).pdf',\n",
       "   'doc_index': 2317,\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'moddate': '2025-07-20T04:41:42+00:00',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'total_pages': 2,\n",
       "   'page_label': '1',\n",
       "   'creationdate': '2025-07-20T04:41:42+00:00',\n",
       "   'keywords': '',\n",
       "   'content_length': 975,\n",
       "   'author': '',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'page': 0,\n",
       "   'source_file': 'paint  (3).pdf'},\n",
       "  'similarity_score': 0.18968689441680908,\n",
       "  'distance': 0.8103131055831909,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_93f7f325_2317',\n",
       "  'content': 'ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational Data Science (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020',\n",
       "  'metadata': {'time_loading': '2025-28-10',\n",
       "   'page': 0,\n",
       "   'trapped': '/False',\n",
       "   'author': '',\n",
       "   'doc_index': 2317,\n",
       "   'moddate': '2025-07-20T04:41:42+00:00',\n",
       "   'page_label': '1',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 975,\n",
       "   'creationdate': '2025-07-20T04:41:42+00:00',\n",
       "   'source_file': 'paint  (3).pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'title': '',\n",
       "   'total_pages': 2,\n",
       "   'source': '..\\\\data\\\\pdf\\\\paint  (3).pdf',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'keywords': '',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.18968689441680908,\n",
       "  'distance': 0.8103131055831909,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_b190d92b_2317',\n",
       "  'content': 'ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational Data Science (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020',\n",
       "  'metadata': {'title': '',\n",
       "   'doc_index': 2317,\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'source': '..\\\\data\\\\pdf\\\\paint  (3).pdf',\n",
       "   'total_pages': 2,\n",
       "   'content_length': 975,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'page': 0,\n",
       "   'moddate': '2025-07-20T04:41:42+00:00',\n",
       "   'author': '',\n",
       "   'trapped': '/False',\n",
       "   'file_type': 'pdf',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'time_loading': '2025-28-10',\n",
       "   'source_file': 'paint  (3).pdf',\n",
       "   'creationdate': '2025-07-20T04:41:42+00:00',\n",
       "   'page_label': '1'},\n",
       "  'similarity_score': 0.18968689441680908,\n",
       "  'distance': 0.8103131055831909,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_f41060dd_2317',\n",
       "  'content': 'ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational Data Science (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020',\n",
       "  'metadata': {'keywords': '',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 0,\n",
       "   'total_pages': 2,\n",
       "   'moddate': '2025-07-20T04:41:42+00:00',\n",
       "   'creationdate': '2025-07-20T04:41:42+00:00',\n",
       "   'content_length': 975,\n",
       "   'time_loading': '2025-28-10',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source': '..\\\\data\\\\pdf\\\\paint  (3).pdf',\n",
       "   'title': '',\n",
       "   'author': '',\n",
       "   'page_label': '1',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'trapped': '/False',\n",
       "   'subject': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'doc_index': 2317,\n",
       "   'source_file': 'paint  (3).pdf'},\n",
       "  'similarity_score': 0.18968689441680908,\n",
       "  'distance': 0.8103131055831909,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_760accd5_2317',\n",
       "  'content': 'ROSHAN CHOWRASIA ♂phone+91-8334913863\\nM.Tech. /envel⌢peroshanchowrasia12@gmail.com\\nComputational Data Science (CDS) Indian Institute Of Science, Bangalore\\n/linkedinLinkedIn Profile\\nSummary\\nSkilled data scientist with a solid foundation in machine learning, deep learning, and natural language\\nprocessing. Proficient in Python, SQL, and frameworks like Scikit-learn and PyTorch. Experienced in\\ndeveloping predictive models, performing statistical analysis, and building end-to-end data pipelines. Adept\\nat deploying data-driven solutions and creating interactive dashboards using Power BI. Strong academic\\nbackground with hands-on experience in projects involving NLP, time series forecasting, and classification\\ntasks.\\nEducation\\n•Indian Institute Of Science, Bangalore 2022-2024\\nM.Tech in Computational Data Science CGPA: 8.6\\n•St Xavier’s College (Autonomous), Kolkata 2020-2022\\nM.Sc. in Computer Science Percentage: 77.62\\n•St Xavier’s College (Autonomous), Kolkata 2017-2020',\n",
       "  'metadata': {'time_loading': '2025-28-10',\n",
       "   'title': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'content_length': 975,\n",
       "   'source': '..\\\\data\\\\pdf\\\\paint  (3).pdf',\n",
       "   'producer': 'pdfTeX-1.40.26',\n",
       "   'subject': '',\n",
       "   'moddate': '2025-07-20T04:41:42+00:00',\n",
       "   'author': '',\n",
       "   'page': 0,\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 2,\n",
       "   'source_file': 'paint  (3).pdf',\n",
       "   'page_label': '1',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2025-07-20T04:41:42+00:00',\n",
       "   'doc_index': 2317},\n",
       "  'similarity_score': 0.18968689441680908,\n",
       "  'distance': 0.8103131055831909,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag_retriever.retrieve(\"What is attention is all you need\")\n",
    "rag_retriever.retrieve(\"Tell me about Roshan Chowrasia IISc Bangalore Computational Data Science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23783e",
   "metadata": {},
   "source": [
    "### RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4b617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40bba05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\", api_key: str =None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model_name=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Format the prompt\n",
    "        formatted_prompt = prompt_template.format(context=context, question=query)\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "        \n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            messages = [HumanMessage(content=simple_prompt)]\n",
    "            response = self.llm.invoke(messages)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc0f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: gemma2-9b-it\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM (you'll need to set GROQ_API_KEY environment variable)\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4110c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Unified Multi-task Learning Framework'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 88.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_bd5cc745_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '3',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'content_length': 941,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_442dbe7b_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'file_type': 'pdf',\n",
       "   'doc_index': 61,\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'keywords': '',\n",
       "   'content_length': 941,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_45e2cfa9_61',\n",
       "  'content': 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       "  'metadata': {'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 941,\n",
       "   'page': 2,\n",
       "   'doc_index': 61,\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'file_type': 'pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'page_label': '3',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'title': 'QZhou-Embedding Technical Report'},\n",
       "  'similarity_score': 0.13801008462905884,\n",
       "  'distance': 0.8619899153709412,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3497d198_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'page': 4,\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'page_label': '5',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'total_pages': 27,\n",
       "   'content_length': 937,\n",
       "   'doc_index': 71,\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_bb00aef7_71',\n",
       "  'content': 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       "  'metadata': {'total_pages': 27,\n",
       "   'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu',\n",
       "   'source': '..\\\\data\\\\pdf\\\\emneddings.pdf',\n",
       "   'creationdate': '2025-09-01T00:50:53+00:00',\n",
       "   'content_length': 937,\n",
       "   'page': 4,\n",
       "   'moddate': '2025-09-01T00:50:53+00:00',\n",
       "   'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "   'title': 'QZhou-Embedding Technical Report',\n",
       "   'arxivid': 'https://arxiv.org/abs/2508.21632v1',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:)',\n",
       "   'doi': 'https://doi.org/10.48550/arXiv.2508.21632',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'source_file': 'emneddings.pdf',\n",
       "   'doc_index': 71,\n",
       "   'page_label': '5',\n",
       "   'keywords': ''},\n",
       "  'similarity_score': 0.10624760389328003,\n",
       "  'distance': 0.89375239610672,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get the context from the retriever and pass it to the LLM\n",
    "\n",
    "rag_retriever.retrieve(\"Unified Multi-task Learning Framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea465ac",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a950a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.0,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df1bf366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism is a technique used in deep learning models to focus on specific parts of the input data that are relevant to the task at hand. It allows the model to selectively weigh the importance of different input elements, rather than treating all elements equally. This is particularly useful in tasks with long-term dependencies, such as the credit assignment problem mentioned in the context, where the model needs to attribute rewards to specific actions or moves.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b1c2",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2832fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Hard Negative Mining Technqiues'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Hard Negative Mining Techniques are used to handle the issue of outliers or misclassified data points in classification problems, particularly when using the exponential error function. These techniques aim to reduce the impact of large negative values of ty(x) on the model's performance.\n",
      "Sources: [{'source': 'paint  (2).pdf', 'page': 681, 'score': -0.3621065616607666, 'preview': 'negative values of ty(x) much more strongly than cross-entropy. In particular, we\\nsee that for large negative values of ty, the cross-entropy grows linearly with |ty|,\\nwhereas the exponential error function grows exponentially with |ty|. Thus the ex-\\nponential error function will be much less robust...'}, {'source': 'paint  (2).pdf', 'page': 681, 'score': -0.3621065616607666, 'preview': 'negative values of ty(x) much more strongly than cross-entropy. In particular, we\\nsee that for large negative values of ty, the cross-entropy grows linearly with |ty|,\\nwhereas the exponential error function grows exponentially with |ty|. Thus the ex-\\nponential error function will be much less robust...'}, {'source': 'paint  (2).pdf', 'page': 681, 'score': -0.3621065616607666, 'preview': 'negative values of ty(x) much more strongly than cross-entropy. In particular, we\\nsee that for large negative values of ty, the cross-entropy grows linearly with |ty|,\\nwhereas the exponential error function grows exponentially with |ty|. Thus the ex-\\nponential error function will be much less robust...'}]\n",
      "Confidence: -0.3621065616607666\n",
      "Context Preview: negative values of ty(x) much more strongly than cross-entropy. In particular, we\n",
      "see that for large negative values of ty, the cross-entropy grows linearly with |ty|,\n",
      "whereas the exponential error function grows exponentially with |ty|. Thus the ex-\n",
      "ponential error function will be much less robust\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"Hard Negative Mining Technqiues\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa6150d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Tell me about roshan chowrasia'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "ROSHAN CHOWRASIA\n",
      "+91-8334913863\n",
      "roshanchowrasia12@gmail.com\n",
      "linkedin.com/in/roshan-chowrasia-625b3625b\n",
      "Summary\n",
      "Data Scientist with 1.5+ years of industrial experience specializing in machine learning, statistical\n",
      "optimization, and time series forecast"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing. Proven expertise in predictive modeling, ETL automation, and data\n",
      "pipeline development using Python, SQL, Pandas, Scikit-learn, and Power BI. Delivered solutions improving\n",
      "P&L by 14.35% and reporting accuracy by 40%. M.Tech. in Computational Data Science from IISc\n",
      "Bangalore.\n",
      "Experience\n",
      "Cloudcraftz Apr 2025 - Oct 2025\n",
      "Data Scientist Kolkata\n",
      "Strategy Performance Reconciliation and Validation:\n",
      "Challenge -Live trading strategies were actively running in the market, and it was essential to validate\n",
      "whether their performance aligned with back-tested expectations.\n",
      "Approach -Leveraged the in-house backtesting engine to re-run strategies on minute-level market data,\n",
      "\n",
      "ROSHAN CHOWRASIA\n",
      "+91-8334913863\n",
      "roshanchowrasia12@gmail.com\n",
      "linkedin.com/in/roshan-chowrasia-625b3625b\n",
      "Summary\n",
      "Data Scientist with 1.5+ years of industrial experience specializing in machine learning, statistical\n",
      "optimization, and time series forecasting. Proven expertise in predictive modeling, ETL automation, and data\n",
      "pipeline development using Python, SQL, Pandas, Scikit-learn, and Power BI. Delivered solutions improving\n",
      "P&L by 14.35% and reporting accuracy by 40%. M.Tech. in Computational Data Science from IISc\n",
      "Bangalore.\n",
      "Experience\n",
      "Cloudcraftz Apr 2025 - Oct 2025\n",
      "Data Scientist Kolkata\n",
      "Strategy Performance Reconciliation and Validation:\n",
      "Challenge -Live trading strategies were actively running in the market, and it was essential to validate\n",
      "whether their performance aligned with back-tested expectations.\n",
      "Approach -Leveraged the in-house backtesting engine to re-run strategies on minute-level market data,\n",
      "\n",
      "ROSHAN CHOWRASIA\n",
      "+91-8334913863\n",
      "roshanchowrasia12@gmail.com\n",
      "linkedin.com/in/roshan-chowrasia-625b3625b\n",
      "Summary\n",
      "Data Scientist with 1.5+ years of industrial experience specializing in machine learning, statistical\n",
      "optimization, and time series forecasting. Proven expertise in predictive modeling, ETL automation, and data\n",
      "pipeline development using Python, SQL, Pandas, Scikit-learn, and Power BI. Delivered solutions improving\n",
      "P&L by 14.35% and reporting accuracy by 40%. M.Tech. in Computational Data Science from IISc\n",
      "Bangalore.\n",
      "Experience\n",
      "Cloudcraftz Apr 2025 - Oct 2025\n",
      "Data Scientist Kolkata\n",
      "Strategy Performance Reconciliation and Validation:\n",
      "Challenge -Live trading strategies were actively running in the market, and it was essential to validate\n",
      "whether their performance aligned with back-tested expectations.\n",
      "Approach -Leveraged the in-house backtesting engine to re-run strategies on minute-level market data,\n",
      "\n",
      "Question: Tell me about roshan chowrasia\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: Roshan Chowrasia is a Data Scientist with 1.5+ years of industrial experience. He specializes in machine learning, statistical optimization, and time series forecasting. He has expertise in predictive modeling, ETL automation, and data pipeline development using various tools and technologies.\n",
      "\n",
      "Citations:\n",
      "[1] Roshan_Chowrasia_Data_Scientist_Resume.pdf (page 0)\n",
      "[2] Roshan_Chowrasia_Data_Scientist_Resume.pdf (page 0)\n",
      "[3] Roshan_Chowrasia_Data_Scientist_Resume.pdf (page 0)\n",
      "Summary: Roshan Chowrasia is a Data Scientist with over 1.5 years of experience, specializing in machine learning, statistical optimization, and time series forecasting. His expertise includes predictive modeling, ETL automation, and data pipeline development using various tools and technologies.\n",
      "History: {'question': 'Tell me about roshan chowrasia', 'answer': 'Roshan Chowrasia is a Data Scientist with 1.5+ years of industrial experience. He specializes in machine learning, statistical optimization, and time series forecasting. He has expertise in predictive modeling, ETL automation, and data pipeline development using various tools and technologies.', 'sources': [{'source': 'Roshan_Chowrasia_Data_Scientist_Resume.pdf', 'page': 0, 'score': -0.25820469856262207, 'preview': 'ROSHAN CHOWRASIA\\n+91-8334913863\\nroshanchowrasia12@gmail.com\\nlinkedin.com/in/roshan-chowrasia-625b3625b\\nSummary\\nData Scie...'}, {'source': 'Roshan_Chowrasia_Data_Scientist_Resume.pdf', 'page': 0, 'score': -0.25820469856262207, 'preview': 'ROSHAN CHOWRASIA\\n+91-8334913863\\nroshanchowrasia12@gmail.com\\nlinkedin.com/in/roshan-chowrasia-625b3625b\\nSummary\\nData Scie...'}, {'source': 'Roshan_Chowrasia_Data_Scientist_Resume.pdf', 'page': 0, 'score': -0.25820469856262207, 'preview': 'ROSHAN CHOWRASIA\\n+91-8334913863\\nroshanchowrasia12@gmail.com\\nlinkedin.com/in/roshan-chowrasia-625b3625b\\nSummary\\nData Scie...'}], 'summary': 'Roshan Chowrasia is a Data Scientist with over 1.5 years of experience, specializing in machine learning, statistical optimization, and time series forecasting. His expertise includes predictive modeling, ETL automation, and data pipeline development using various tools and technologies.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"Tell me about roshan chowrasia\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d695e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roshan\\Desktop\\RAG\\RAG-Tutorials\\notebook\\data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(Path(\"data\").resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
